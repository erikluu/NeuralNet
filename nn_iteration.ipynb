{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Effects</th>\n",
       "      <th>Flavor</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Energetic</th>\n",
       "      <th>Tingly</th>\n",
       "      <th>Euphoric</th>\n",
       "      <th>Relaxed</th>\n",
       "      <th>...</th>\n",
       "      <th>Ammonia</th>\n",
       "      <th>Minty</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Fruit</th>\n",
       "      <th>Butter</th>\n",
       "      <th>Pineapple</th>\n",
       "      <th>Tar</th>\n",
       "      <th>Rose</th>\n",
       "      <th>Plum</th>\n",
       "      <th>Pear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100-Og</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
       "      <td>Earthy,Sweet,Citrus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98-White-Widow</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
       "      <td>Flowery,Violet,Diesel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>sativa</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
       "      <td>Spicy/Herbal,Sage,Woody</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13-Dawgs</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
       "      <td>Apricot,Citrus,Grapefruit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24K-Gold</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
       "      <td>Citrus,Earthy,Orange</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Strain    Type  Rating                                     Effects  \\\n",
       "0          100-Og  hybrid     4.0  Creative,Energetic,Tingly,Euphoric,Relaxed   \n",
       "1  98-White-Widow  hybrid     4.7    Relaxed,Aroused,Creative,Happy,Energetic   \n",
       "2            1024  sativa     4.4   Uplifted,Happy,Relaxed,Energetic,Creative   \n",
       "3        13-Dawgs  hybrid     4.2     Tingly,Creative,Hungry,Relaxed,Uplifted   \n",
       "4        24K-Gold  hybrid     4.6   Happy,Relaxed,Euphoric,Uplifted,Talkative   \n",
       "\n",
       "                      Flavor  Creative  Energetic  Tingly  Euphoric  Relaxed  \\\n",
       "0        Earthy,Sweet,Citrus       1.0        1.0     1.0       1.0      1.0   \n",
       "1      Flowery,Violet,Diesel       1.0        1.0     0.0       0.0      1.0   \n",
       "2    Spicy/Herbal,Sage,Woody       1.0        1.0     0.0       0.0      1.0   \n",
       "3  Apricot,Citrus,Grapefruit       1.0        0.0     1.0       0.0      1.0   \n",
       "4       Citrus,Earthy,Orange       0.0        0.0     0.0       1.0      1.0   \n",
       "\n",
       "   ...  Ammonia  Minty  Tree  Fruit  Butter  Pineapple  Tar  Rose  Plum  Pear  \n",
       "0  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "1  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "2  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "3  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "4  ...      0.0    0.0   0.0    0.0     0.0        0.0  0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cannabis_df = pd.read_csv(\"cannabis_full.csv\")\n",
    "cannabis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, target_column, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    num_test = int(len(data) * test_ratio)\n",
    "    \n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_indices = shuffled_indices[:num_test]\n",
    "    train_indices = shuffled_indices[num_test:]\n",
    "\n",
    "    train_df = data.iloc[train_indices]\n",
    "    test_df = data.iloc[test_indices]\n",
    "\n",
    "    X_train = train_df.drop(target_column, axis=1)\n",
    "    y_train = train_df[target_column]\n",
    "\n",
    "    X_test = test_df.drop(target_column, axis=1)\n",
    "    y_test = test_df[target_column]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = cannabis_df.dropna()\n",
    "c_df = c_df[c_df[\"Type\"].isin([\"sativa\", \"indica\"])]\n",
    "c_df[\"Type_is_sativa\"] = np.where(c_df[\"Type\"] == \"sativa\", 1, 0)\n",
    "c_df = c_df.drop([\"Strain\", \"Effects\", \"Flavor\", \"Type\"], axis=1)\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_dataset(c_df, target_column=\"Type_is_sativa\", test_ratio=0.2)\n",
    "X_train[\"intercept\"] = 1\n",
    "X_test[\"intercept\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, yhat):\n",
    "    true_positives = np.sum((y == 1) & (yhat == 1))\n",
    "    predicted_positives = np.sum(yhat == 1)\n",
    "    return true_positives / predicted_positives\n",
    "\n",
    "def recall(y, yhat):\n",
    "    true_positives = np.sum((y == 1) & (yhat == 1))\n",
    "    actual_positives = np.sum(y == 1)\n",
    "    return true_positives / actual_positives\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "\n",
    "def squared_error(y, yhat):\n",
    "    n = len(y)\n",
    "    l = 1 / n * np.power(np.sum(yhat - y), 2)\n",
    "    return l\n",
    "\n",
    "def svc_hinge_loss(y, yhat):\n",
    "    n = len(y)\n",
    "    l = 1 / n * np.sum(np.where(y * yhat) > 1, y * yhat, 0)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Functions\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    left = np.power(1 / (1 + np.exp(-z)), 2)\n",
    "    right = -np.exp(-z)\n",
    "    return left * right\n",
    "\n",
    "def relu_grad(u):\n",
    "    return u if u > 0 else 0\n",
    "\n",
    "def softplus_grad(n):\n",
    "    num = np.exp(n)\n",
    "    den = np.exp(n) + 1\n",
    "    return num / den\n",
    "\n",
    "def squared_error_grad(y, yhat):\n",
    "    n = len(y)\n",
    "    dl = 2 / n * np.sum(yhat - y)\n",
    "    return dl\n",
    "\n",
    "def svc_hinge_grad(y, yhat):\n",
    "    n = len(y)\n",
    "    dl = 1 / n * np.sum(np.where(y * yhat) > 1, yhat, 0)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W, V, activation_f):\n",
    "    Z = X.dot(W)\n",
    "    Q = activation_f(Z)\n",
    "    n = Q.dot(V)\n",
    "    p = activation_f(n)\n",
    "    \n",
    "    return Z, Q, n, p\n",
    "\n",
    "\n",
    "def backpropogation(v, eta, activation_gradient_f, loss_gradient_f):\n",
    "    dl = loss_gradient_f(v['y'], v['p'])\n",
    "    dp = activation_gradient_f(v['n'])\n",
    "    dn = v['Q']\n",
    "    V_i = (dl * dp).dot(dn)\n",
    "\n",
    "    dv = activation_gradient_f(v['Z'])\n",
    "    # dv = v['Z']\n",
    "    dz = v['X']\n",
    "    W_i = ((V_i * dv).T @ dz).T\n",
    "\n",
    "    assert v['W'].shape == W_i.shape\n",
    "    assert v['V'].shape == V_i.shape\n",
    "\n",
    "    V = v['V'] + (V_i * eta)\n",
    "    W = v['W'] + (W_i * eta)\n",
    "\n",
    "    return W, V\n",
    "\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n",
    "# atol - absolute tolerance (additive)\n",
    "# rtol - relative tolerance (multiplicative)\n",
    "# atol more important than rtol here since values are small?\n",
    "def stopping_condition(W, W_new, V, V_new):\n",
    "    return np.allclose(W, W_new, atol=0.5, rtol=0.1) and np.allclose(V, V_new, atol=0.5, rtol=0.1)\n",
    "\n",
    "\n",
    "def fit(X, y, eta, activation_funcs, loss_funcs, max_iter=1e6):\n",
    "    W = np.random.random((X.shape[1], 3))\n",
    "    V = np.random.random(3)\n",
    "    for _ in trange(int(max_iter)):\n",
    "        Z, Q, n, p = forward_pass(X, W, V, activation_funcs['activation_f'])\n",
    "\n",
    "        values = {'W': W, 'V': V, 'Z': Z, 'Q': Q, 'n': n, 'p': p, 'X': X, 'y': y}\n",
    "\n",
    "        W_new, V_new = backpropogation(values, eta, activation_funcs['gradient_f'], loss_funcs['gradient_f'])\n",
    "\n",
    "        while not stopping_condition(W, W_new, V, V_new):\n",
    "            print(\"stopping\")\n",
    "            return W_new, V_new\n",
    "        \n",
    "        W = W_new\n",
    "        V = V_new\n",
    "\n",
    "    return W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_funs = {'activation_f': relu, 'gradient_f': relu_grad}\n",
    "sigmoid_funcs = {'activation_f': sigmoid, 'gradient_f': sigmoid_grad}\n",
    "softplus_funcs = {'activation_f': softplus, 'gradient_f': softplus_grad}\n",
    "\n",
    "squared_error_funcs = {'loss_f': squared_error, 'gradient_f': squared_error_grad}\n",
    "svc_hinge_funcs = {'loss_f': svc_hinge_loss, 'gradient_f': svc_hinge_grad}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\n",
    "def relu(x):\n",
    "    return torch.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def softplus(x):\n",
    "    return torch.log(1 + torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "\n",
    "def squared_error(y, yhat):\n",
    "    n = len(y)\n",
    "    l = 1 / n * torch.pow(torch.sum(yhat - y), 2)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Functions\n",
    "\n",
    "def squared_error_grad(y, yhat):\n",
    "    n = len(y)\n",
    "    dl = 2 / n * torch.sum(yhat - y)\n",
    "    return dl\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    left = torch.pow(1 / (1 + torch.exp(-z)), 2)\n",
    "    right = -torch.exp(-z)\n",
    "    return left * right\n",
    "\n",
    "def softplus_grad(n):\n",
    "    num = torch.exp(n)\n",
    "    den = torch.exp(n) + 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1840/5000000 [00:16<12:14:04, 113.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m sigmoid_funcs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mactivation_f\u001b[39m\u001b[39m'\u001b[39m: sigmoid, \u001b[39m'\u001b[39m\u001b[39mgradient_f\u001b[39m\u001b[39m'\u001b[39m: sigmoid_grad}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m squared_error_funcs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mloss_f\u001b[39m\u001b[39m'\u001b[39m: squared_error, \u001b[39m'\u001b[39m\u001b[39mgradient_f\u001b[39m\u001b[39m'\u001b[39m: squared_error_grad}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m W, V \u001b[39m=\u001b[39m fit(X_train\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues, eta\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, activation_funcs\u001b[39m=\u001b[39;49msigmoid_funcs, loss_funcs\u001b[39m=\u001b[39;49msquared_error_funcs, max_iter\u001b[39m=\u001b[39;49m\u001b[39m5e6\u001b[39;49m)\n",
      "\u001b[1;32m/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m trange(\u001b[39mint\u001b[39m(max_iter)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     Z, Q, n, p \u001b[39m=\u001b[39m forward_pass(X, W, V, activation_funcs[\u001b[39m'\u001b[39;49m\u001b[39mactivation_f\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     values \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mW\u001b[39m\u001b[39m'\u001b[39m: W, \u001b[39m'\u001b[39m\u001b[39mV\u001b[39m\u001b[39m'\u001b[39m: V, \u001b[39m'\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m'\u001b[39m: Z, \u001b[39m'\u001b[39m\u001b[39mQ\u001b[39m\u001b[39m'\u001b[39m: Q, \u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m: n, \u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m: p, \u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m: X, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m: y}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     W_new, V_new \u001b[39m=\u001b[39m backpropogation(values, eta, activation_funcs[\u001b[39m'\u001b[39m\u001b[39mgradient_f\u001b[39m\u001b[39m'\u001b[39m], loss_funcs[\u001b[39m'\u001b[39m\u001b[39mgradient_f\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_pass\u001b[39m(X, W, V, activation_f):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     Z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(X, W)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     Q \u001b[39m=\u001b[39m activation_f(Z)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erikluu/Documents/Poly/DATA402/NeuralNet/nn_iteration.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     n \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(Q, V)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def forward_pass(X, W, V, activation_f):\n",
    "    Z = torch.matmul(X, W)\n",
    "    Q = activation_f(Z)\n",
    "    n = torch.matmul(Q, V)\n",
    "    p = activation_f(n)\n",
    "    \n",
    "    return Z, Q, n, p\n",
    "\n",
    "\n",
    "def backpropogation(v, eta, activation_gradient_f, loss_gradient_f):\n",
    "    dl = loss_gradient_f(v['y'], v['p'])\n",
    "    dp = activation_gradient_f(v['n'])\n",
    "    dn = v['Q']\n",
    "    V_i = (dl * dp).matmul(dn)\n",
    "\n",
    "    dv = activation_gradient_f(v['Z'])\n",
    "    dz = v['X']\n",
    "    W_i =  (V_i * dv).t().matmul(dz).t()\n",
    "\n",
    "    assert v['W'].shape == W_i.shape\n",
    "    assert v['V'].shape == V_i.shape\n",
    "\n",
    "    V = v['V'] + (V_i * eta)\n",
    "    W = v['W'] + (W_i * eta)\n",
    "\n",
    "    return W, V\n",
    "\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n",
    "# atol - absolute tolerance (additive)\n",
    "# rtol - relative tolerance (multiplicative)\n",
    "# atol more important than rtol here since values are small?\n",
    "def stopping_condition(W, W_new, V, V_new):\n",
    "    return torch.allclose(W, W_new, atol=1, rtol=1) and torch.allclose(V, V_new, atol=1, rtol=1)\n",
    "\n",
    "\n",
    "def fit(X, y, eta, activation_funcs, loss_funcs, max_iter=1e6):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "    W = torch.rand((X.shape[1], 3), device=device)\n",
    "    V = torch.rand(3, device=device)\n",
    "    for _ in trange(int(max_iter)):\n",
    "        Z, Q, n, p = forward_pass(X, W, V, activation_funcs['activation_f'])\n",
    "\n",
    "        values = {'W': W, 'V': V, 'Z': Z, 'Q': Q, 'n': n, 'p': p, 'X': X, 'y': y}\n",
    "\n",
    "        W_new, V_new = backpropogation(values, eta, activation_funcs['gradient_f'], loss_funcs['gradient_f'])\n",
    "\n",
    "        while not stopping_condition(W, W_new, V, V_new):\n",
    "            print(\"stopping\")\n",
    "            return W_new, V_new\n",
    "        \n",
    "        W = W_new\n",
    "        V = V_new\n",
    "\n",
    "    return W, V\n",
    "\n",
    "sigmoid_funcs = {'activation_f': sigmoid, 'gradient_f': sigmoid_grad}\n",
    "squared_error_funcs = {'loss_f': squared_error, 'gradient_f': squared_error_grad}\n",
    "W, V = fit(X_train.values, y_train.values, eta=0.001, activation_funcs=sigmoid_funcs, loss_funcs=squared_error_funcs, max_iter=5e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN with Squared-Error Loss\n",
    "\n",
    "CHECK DOUBLE SIGMOID... sus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2788.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "W, V = fit(X_train.values, y_train.values, eta=0.0001, activation_funcs=sigmoid_funcs, loss_funcs=squared_error_funcs, max_iter=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/16/w2tjk8ms4t322jq3ms1b49rh0000gn/T/ipykernel_77632/1174609915.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return true_positives / actual_positives\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, nan, nan)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, p = forward_pass(X_test.values, W, V, sigmoid)\n",
    "predictions = np.where(p > 0.5, 1, 0)\n",
    "\n",
    "pr = precision(predictions, y_test.values)\n",
    "re = recall(predictions, y_test.values)\n",
    "f1 = f1_score(pr, re)\n",
    "pr, re, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3798647419819812, 0.3648935234972355)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.max(), p.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN with SVC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_funcs = {'activation_f': sigmoid, 'gradient_f': sigmoid_grad}\n",
    "svc_funcs = {'loss_f': squared_error, 'gradient_f': squared_error_grad}\n",
    "W, V = fit(X_train.values, y_train.values, eta=0.00001, activation_funcs=sigmoid_funcs, loss_funcs=squared_error_funcs, max_iter=1e6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data403",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
